{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn import covariance\n",
    "import scipy\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import Bounds\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# universal, no matter the sample\n",
    "%store -r model_ewma_5years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "Due to the large number of stocks and high computational cost, calculation for sample 6 has been performed in a separate notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample0 = ['AXP', 'BA', 'CAT', 'DD', 'DIS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM','KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'PG', 'UTX', 'WMT', 'XOM', '^GSPC']\n",
    "\n",
    "sample1 = ['AAPL', 'ABC', 'ABMD', 'ABT', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK','AEE', 'AEP', 'AES', 'AFL', 'AGN', 'AIG', 'AIV', 'AJG', 'ALB', 'ALK',\n",
    "        'ALL', 'ALXN', 'AMAT', 'AMD', 'AME', 'AMGN', 'AMT', 'AMZN', 'ANSS', 'AON', 'AOS', 'APA', 'APD', 'APH', 'ARE', 'ARNC', 'ATO', 'ATVI', 'AVB',\n",
    "        'AVY', 'AXP', 'AZO', 'BA', 'BAC', 'BAX', 'BBY', 'BDX', 'BEN', 'BIIB','BK', 'BKR', '^GSPC']\n",
    "\n",
    "sample2 = ['BLL', 'BMY', 'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT', 'CB','CCI', 'CCL', 'CDNS', 'CERN', 'CHD', 'CHRW', 'CI', 'CINF', 'CL', 'CLX',\n",
    "        'CMA', 'CMCSA', 'CMI', 'CMS', 'CNP', 'COF', 'COG', 'COO', 'COP', 'COST', 'CPB', 'CPRT', 'CSCO', 'CSX', 'CTAS', 'CTL', 'CTSH', 'CTXS', 'CVS',\n",
    "        'CVX', 'D', 'DD', 'DE', 'DGX', 'DHI', 'DHR', 'DIS', 'DISH', 'DLTR', 'DOV', '^GSPC']\n",
    "\n",
    "sample3 = ['DRE', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'EA', 'EBAY', 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'EOG', 'EQR', 'ES', 'ESS',\n",
    "        'ETFC', 'ETN', 'ETR', 'EVRG', 'EXC', 'EXPD', 'F', 'FAST', 'FCX', 'FDX','FE', 'FISV', 'FITB', 'FLIR', 'FLS', 'FMC', 'FRT', 'GD', 'GE', 'GILD',\n",
    "        'GIS', 'GL', 'GLW', 'GPC', 'GPS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HD', '^GSPC']\n",
    "\n",
    "sample4 = ['AAPL', 'ABC', 'ABMD', 'ABT', 'ADBE', 'ADI', 'ADM', 'ADP', 'ADSK', 'AEE', 'AEP', 'AES', 'AFL', 'AGN', 'AIG', 'AIV', 'AJG', 'ALB', 'ALK',\n",
    "       'ALL', 'ALXN', 'AMAT', 'AMD', 'AME', 'AMGN', 'AMT', 'AMZN', 'ANSS', 'AON', 'AOS', 'APA', 'APD', 'APH', 'ARE', 'ARNC', 'ATO', 'ATVI', 'AVB',\n",
    "       'AVY', 'AXP', 'AZO', 'BA', 'BAC', 'BAX', 'BBY', 'BDX', 'BEN', 'BIIB','BK', 'BKR', 'HES', 'HFC', 'HIG', 'HOG', 'HOLX', 'HON', 'HPQ', 'HRB',\n",
    "       'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'IBM', 'IDXX', 'IEX', 'IFF', 'INCY', 'INTC', 'INTU', 'IP', 'IPG', 'IR', 'IRM', 'IT', 'ITW', 'IVZ', 'J',\n",
    "       'JBHT', 'JCI', 'JKHY', 'JNJ', 'JPM', 'JWN', 'K', 'KEY', 'KIM', 'KLAC','KMB', 'KMX', 'KO', 'KR', 'KSS', 'KSU', 'L', 'LB', 'LEG', 'LEN', 'LH',\n",
    "       'LHX','LIN', 'LLY', 'LMT', 'LNC', 'LNT', 'LOW', 'LRCX', 'LUV', 'M', 'MAA','MAR', 'MAS', 'MCD', 'MCHP', 'MCK', 'MCO', 'MDT', 'MGM', 'MHK', 'MKC',\n",
    "       'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS', 'MRK', 'MRO', 'MS', 'MSFT', 'MSI', 'MTB', 'MTD', 'MU', 'MXIM', 'MYL', 'NBL', 'NEE', 'NEM', 'NI',\n",
    "       'NKE', 'NLOK', 'NOC', 'NOV', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVR', 'NWL','^GSPC']\n",
    "\n",
    "sample5 = ['BLL', 'BMY', 'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT', 'CB','CCI', 'CCL', 'CDNS', 'CERN', 'CHD', 'CHRW', 'CI', 'CINF', 'CL', 'CLX',\n",
    "       'CMA', 'CMCSA', 'CMI', 'CMS', 'CNP', 'COF', 'COG', 'COO', 'COP', 'COST','CPB', 'CPRT', 'CSCO', 'CSX', 'CTAS', 'CTL', 'CTSH', 'CTXS', 'CVS',\n",
    "       'CVX', 'D', 'DD', 'DE', 'DGX', 'DHI', 'DHR', 'DIS', 'DISH', 'DLTR','DOV', 'O', 'ODFL', 'OKE', 'OMC', 'ORCL', 'ORLY', 'OXY', 'PAYX', 'PBCT',\n",
    "       'PCAR', 'PEAK', 'PEG', 'PEP', 'PFE', 'PG', 'PGR', 'PH', 'PHM', 'PKI','PLD', 'PNC', 'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PSA', 'PVH', 'PWR',\n",
    "       'PXD', 'QCOM', 'RCL', 'RE', 'REG', 'REGN', 'RF', 'RHI', 'RJF', 'RL','RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTN', 'SBUX', 'SCHW', 'SEE',\n",
    "       'SHW', 'SIVB', 'SJM', 'SLB', 'SLG', 'SNA', 'SNPS', 'SO', 'SPG', 'SPGI', 'SRE','STE', 'STT', 'STZ', 'SWK', 'SWKS', 'SYK', 'SYY', 'T', 'TAP', 'TFC',\n",
    "       'TFX', 'TGT', 'TIF', 'TJX', 'TMO', 'TROW', 'TRV', 'TSCO', 'TSN', 'TTWO','TXN', 'TXT', 'UDR', 'UHS', 'UNH', 'UNM', 'UNP', 'URI', 'USB', 'UTX',\n",
    "       'VAR', 'VFC', 'VLO', 'VMC', 'VNO', 'VRSN', 'VRTX', 'VTR', 'VZ', 'WAB','^GSPC']\n",
    "\n",
    "sample6 = [\"AAPL\",\t\"ABC\",\t\"ABMD\",\t\"ABT\",\t\"ADBE\",\t\"ADI\",\t\"ADM\",\t\"ADP\",\t\"ADSK\",\t\"AEE\",\t\"AEP\",\t\"AES\",\t\"AFL\",\t\"AGN\",\t\"AIG\",\t\"AIV\",\t\"AJG\",\t\"ALB\",\t\"ALK\",\t\"ALL\",\t\"ALXN\",\t\"AMAT\",\t\"AMD\",\t\"AME\",\t\"AMGN\",\t\"AMT\",\t\"AMZN\",\t\"ANSS\",\t\"AON\",\t\"AOS\",\t\"APA\",\t\"APD\",\t\"APH\",\t\"ARE\",\t\"ARNC\",\t\"ATO\",\t\"ATVI\",\t\"AVB\",\t\"AVY\",\t\"AXP\",\t\"AZO\",\t\"BA\",\t\"BAC\",\t\"BAX\",\t\"BBY\",\t\"BDX\",\t\"BEN\",\t\"BIIB\",\t\"BK\",\t\"BKR\",\t\"BLL\",\t\"BMY\",\t\"BSX\",\t\"BWA\",\t\"BXP\",\t\"C\",\t\"CAG\",\t\"CAH\",\t\"CAT\",\t\"CB\",\t\"CCI\",\t\"CCL\",\t\"CDNS\",\t\"CERN\",\t\"CHD\",\t\"CHRW\",\t\"CI\",\t\"CINF\",\t\"CL\",\t\"CLX\",\t\"CMA\",\t\"CMCSA\",\t\"CMI\",\t\"CMS\",\t\"CNP\",\t\"COF\",\t\"COG\",\t\"COO\",\t\"COP\",\t\"COST\",\t\"CPB\",\t\"CPRT\",\t\"CSCO\",\t\"CSX\",\t\"CTAS\",\t\"CTL\",\t\"CTSH\",\t\"CTXS\",\t\"CVS\",\t\"CVX\",\t\"D\",\t\"DD\",\t\"DE\",\t\"DGX\",\t\"DHI\",\t\"DHR\",\t\"DIS\",\t\"DISH\",\t\"DLTR\",\t\"DOV\",\t\"DRE\",\t\"DRI\",\t\"DTE\",\t\"DUK\",\t\"DVA\",\t\"DVN\",\t\"DXC\",\t\"EA\",\t\"EBAY\",\t\"ECL\",\t\"ED\",\t\"EFX\",\t\"EIX\",\t\"EL\",\t\"EMN\",\t\"EMR\",\t\"EOG\",\t\"EQR\",\t\"ES\",\t\"ESS\",\t\"ETFC\",\t\"ETN\",\t\"ETR\",\t\"EVRG\",\t\"EXC\",\t\"EXPD\",\t\"F\",\t\"FAST\",\t\"FCX\",\t\"FDX\",\t\"FE\",\t\"FISV\",\t\"FITB\",\t\"FLIR\",\t\"FLS\",\t\"FMC\",\t\"FRT\",\t\"GD\",\t\"GE\",\t\"GILD\",\t\"GIS\",\t\"GL\",\t\"GLW\",\t\"GPC\",\t\"GPS\",\t\"GWW\",\t\"HAL\",\t\"HAS\",\t\"HBAN\",\t\"HD\",\t\"HES\",\t\"HFC\",\t\"HIG\",\t\"HOG\",\t\"HOLX\",\t\"HON\",\t\"HPQ\",\t\"HRB\",\t\"HRL\",\t\"HSIC\",\t\"HST\",\t\"HSY\",\t\"HUM\",\t\"IBM\",\t\"IDXX\",\t\"IEX\",\t\"IFF\",\t\"INCY\",\t\"INTC\",\t\"INTU\", \"IP\",\t\"IPG\",\t\"IR\",\t\"IRM\",\t\"IT\",\t\"ITW\",\t\"IVZ\",\t\"J\",\t\"JBHT\",\t\"JCI\",\t\"JKHY\",\t\"JNJ\",\t\"JPM\",\t\"JWN\",\t\"K\",\t\"KEY\",\t\"KIM\",\t\"KLAC\",\t\"KMB\",\t\"KMX\",\t\"KO\",\t\"KR\",\t\"KSS\",\t\"KSU\",\t\"L\",\t\"LB\",\t\"LEG\",\t\"LEN\",\t\"LH\",\t\"LHX\",\t\"LIN\",\t\"LLY\",\t\"LMT\",\t\"LNC\",\t\"LNT\",\t\"LOW\",\t\"LRCX\",\t\"LUV\",\t\"M\",\t\"MAA\",\t\"MAR\",\t\"MAS\",\t\"MCD\",\t\"MCHP\",\t\"MCK\",\t\"MCO\",\t\"MDT\",\t\"MGM\",\t\"MHK\",\t\"MKC\",\t\"MLM\",\t\"MMC\",\t\"MMM\",\t\"MNST\",\t\"MO\",\t\"MOS\",\t\"MRK\",\t\"MRO\",\t\"MS\",\t\"MSFT\",\t\"MSI\",\t\"MTB\",\t\"MTD\",\t\"MU\",\t\"MXIM\",\t\"MYL\",\t\"NBL\",\t\"NEE\",\t\"NEM\",\t\"NI\",\t\"NKE\",\t\"NLOK\",\t\"NOC\",\t\"NOV\",\t\"NSC\",\t\"NTAP\",\t\"NTRS\",\t\"NUE\",\t\"NVR\",\t\"NWL\",\t\"O\",\t\"ODFL\",\t\"OKE\",\t\"OMC\",\t\"ORCL\",\t\"ORLY\",\t\"OXY\",\t\"PAYX\",\t\"PBCT\",\t\"PCAR\",\t\"PEAK\",\t\"PEG\",\t\"PEP\",\t\"PFE\",\t\"PG\",\t\"PGR\",\t\"PH\",\t\"PHM\",\t\"PKI\",\t\"PLD\",\t\"PNC\",\t\"PNR\",\t\"PNW\",\t\"PPG\",\t\"PPL\",\t\"PRGO\",\t\"PSA\",\t\"PVH\",\t\"PWR\",\t\"PXD\",\t\"QCOM\",\t\"RCL\",\t\"RE\",\t\"REG\",\t\"REGN\",\t\"RF\",\t\"RHI\",\t\"RJF\",\t\"RL\",\t\"RMD\",\t\"ROK\",\t\"ROL\",\t\"ROP\",\t\"ROST\",\t\"RSG\",\t\"RTN\",\t\"SBUX\",\t\"SCHW\",\t\"SEE\",\t\"SHW\",\t\"SIVB\",\t\"SJM\",\t\"SLB\",\t\"SLG\",\t\"SNA\",\t\"SNPS\",\t\"SO\",\t\"SPG\",\t\"SPGI\",\t\"SRE\",\t\"STE\",\t\"STT\",\t\"STZ\",\t\"SWK\",\t\"SWKS\",\t\"SYK\",\t\"SYY\",\t\"T\",\t\"TAP\",\t\"TFC\",\t\"TFX\",\t\"TGT\",\t\"TIF\",\t\"TJX\",\t\"TMO\",\t\"TROW\",\t\"TRV\",\t\"TSCO\",\t\"TSN\",\t\"TTWO\",\t\"TXN\",\t\"TXT\",\t\"UDR\",\t\"UHS\",\t\"UNH\",\t\"UNM\",\t\"UNP\",\t\"URI\",\t\"USB\",\t\"UTX\",\t\"VAR\",\t\"VFC\",\t\"VLO\",\t\"VMC\",\t\"VNO\",\t\"VRSN\",\t\"VRTX\",\t\"VTR\",\t\"VZ\",\t\"WAB\",\t\"WAT\",\t\"WBA\",\t\"WDC\",\t\"WEC\",\t\"WELL\",\t\"WFC\",\t\"WHR\",\t\"WM\",\t\"WMB\",\t\"WMT\",\t\"WRB\",\t\"WY\",\t\"XEL\",\t\"XLNX\",\t\"XOM\",\t\"XRAY\",\t\"XRX\",\t\"YUM\",\t\"ZBRA\",\t\"ZION\",\t\"^GSPC\",]\n",
    "\n",
    "sample7 = ['XLY', 'XLP', 'XLE', 'XLF', 'XLV', 'XLB', 'XLU', 'IBB', 'IYW', 'SOXX', 'IYJ', 'IYZ', '^GSPC']\n",
    "\n",
    "sample8 = ['IVV', 'IWF', 'IJH', 'IJR', 'IWD', 'IWM', 'IVW', 'IWB', 'IVE', 'IWV','IUSG', 'IWO', 'IWN', 'IJK', 'IYW', 'IJS', 'IJJ', 'IJT', 'IYH', 'IYF',\n",
    "        'IDU', 'IYY', 'IYG', 'IYC', 'IYJ', 'IYK', 'IYZ', 'IYE', 'IYM', '^GSPC']\n",
    "\n",
    "samples = [sample0, sample1, sample2, sample3, sample4, sample5, sample7, sample8] #include sample 6 at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 30)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample7), len(sample8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Data//df_371.xlsx')\n",
    "df = df.set_index('Date')\n",
    "df_CCR = (np.log(df)-np.log(df.shift(1)))[1:]\n",
    "df_CCR = df_CCR[df_CCR.index.year<2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sector ETF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ETF = pd.read_excel('Data//ETF_sample.xlsx')\n",
    "df_ETF = df_ETF.set_index('Date')\n",
    "df_CCR_ETF = (np.log(df_ETF)-np.log(df_ETF.shift(1)))[1:]\n",
    "df_CCR_ETF = df_CCR_ETF[df_CCR_ETF.index.year<2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iShares ETF Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ETF1 = pd.read_excel('Data//iShares_etfs.xlsx')\n",
    "df_ETF1 = df_ETF1.set_index('Date')\n",
    "df_CCR_ETF1 = (np.log(df_ETF1)-np.log(df_ETF1.shift(1)))[1:]\n",
    "df_CCR_ETF1 = df_CCR_ETF1[df_CCR_ETF1.index.year<2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing All Data in One Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for j in range (len(samples)):\n",
    "    bln = []\n",
    "    for i in range (len(df_CCR.columns)):\n",
    "        if np.asarray([df_CCR.columns[i] == samples[j][z] for z in range (len(samples[j]))]).any() == True:\n",
    "            bln.append (np.bool(1))\n",
    "        else:\n",
    "            bln.append(np.bool(0))\n",
    "    data[j] = df_CCR[df_CCR.columns[bln]]  \n",
    "    \n",
    "data[6] = df_CCR_ETF\n",
    "data[7] = df_CCR_ETF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 51, 51, 51, 151, 151, 13, 30]\n"
     ]
    }
   ],
   "source": [
    "print ([len(data[i].columns) for i in range (len(data))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of rebalancing intervals \n",
    "delta = 21\n",
    "\n",
    "#T = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Startig Dates For Each Data Sample\n",
    "Except for the ETF samples our backtesting starts from 01 February 2000.\n",
    "We perform monthly rebalancing (21 trading days)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "starts = [dt.datetime(2000, 2, 1), dt.datetime(2000, 2, 1), dt.datetime(2000, 2, 1), dt.datetime(2000, 2, 1), dt.datetime(2000, 2, 1), dt.datetime(2000, 2, 1), \n",
    "          dt.datetime(2003, 1, 2), dt.datetime(2002, 1, 2)]\n",
    "\n",
    "starting_dates = {}\n",
    "for i in range(len(data)):\n",
    "    total=0\n",
    "    dates = []\n",
    "    while total<len(data[i][data[i].index>starts[i]]):\n",
    "        dates.append(data[i].index[int(np.argwhere(data[i].index==starts[i]))+total])\n",
    "        total+=delta\n",
    "    starting_dates[i] = pd.to_datetime(dates)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Slope & Mean Squared Error\n",
    "Prior to each rebalancing date we have to calculate beta and MSE for each stock in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_parameters (T, df_CCR, starting_dates, market_index):\n",
    "    \"\"\"\"\"\n",
    "    T - time period used for estimation (nubmer of trading days)\n",
    "    df_CCR - continiously compounded data, having market index placed in the last column\n",
    "    starting_dates - rebalancing periods\n",
    "    market_index - name of the last column (log return of the market index)\n",
    "    \n",
    "    the function returns beta and annualized MSE\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    betas = pd.DataFrame(data=None, index=np.arange(len(starting_dates)), columns=df_CCR.columns[:-1])\n",
    "    MSE = pd.DataFrame(data=None, index=np.arange(len(starting_dates)), columns=df_CCR.columns[:-1])\n",
    "\n",
    "    for i in range (len(starting_dates)):\n",
    "        for z in range (len(df_CCR.columns)-1):\n",
    "            x = df_CCR[(df_CCR.index<starting_dates[i])*(df_CCR.index>df_CCR.index[int(np.argwhere(df_CCR.index==starting_dates[i])-T-1)])][market_index].values.reshape(-1, 1)\n",
    "            y = df_CCR[(df_CCR.index<starting_dates[i])*(df_CCR.index>df_CCR.index[int(np.argwhere(df_CCR.index==starting_dates[i])-T-1)])][df_CCR.columns[z]].values\n",
    "            model = LinearRegression()\n",
    "            model.fit(x, y)            \n",
    "            betas.iloc[i:i+1, z] = model.coef_            \n",
    "            y_pred = model.predict(x)            \n",
    "            MSE.iloc[i:i+1, z] = mean_squared_error(y, y_pred) \n",
    "            \n",
    "    MSE = MSE*252\n",
    "            \n",
    "    return betas, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using 60 trading days (approximately 3 months) prior to each rebalancing interval to estimate the parameters. Afterwards we store beta and MSE values for all samples in their respective dictionaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_index = '^GSPC'\n",
    "betas = {}\n",
    "MSE = {}\n",
    "for i in range(len(data)):\n",
    "    res = regression_parameters (60, data[i], starting_dates[i], market_index)\n",
    "    betas[i], MSE[i] = res[0], res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance-Covariance Matrices - Market Model\n",
    "\n",
    "    covariance between stocks:\n",
    "    beta*beta*var(m)\n",
    "    \n",
    "    individual variance: \n",
    "    beta^2 * var(m) + MSE \n",
    "    optimization automatically punishes the securities where market model estimation error is larger, since it adds MSE to their volatility estimate\n",
    "    * variance and MSE should be annualized, whereas beta is standardized parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_cov (vol, betas, MSE, start):\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    annualized volatility should be in decimals, e.g. 25% should be written as 0.25\n",
    "    start - in order to make sure all volatilities have the same starting date\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    vol = vol[vol.index>=start]\n",
    "    vol = np.asarray(vol)\n",
    "    \n",
    "    variance = pd.DataFrame(data=None, index=betas.index, columns=betas.columns)\n",
    "    for i in range (len(vol)):\n",
    "        for z in range (len(betas.columns)):\n",
    "            variance.iloc[i:i+1, z] = (float(betas.iloc[i:i+1, z]))**2 * vol[i]**2 + MSE.iloc[i:i+1, z]\n",
    "\n",
    "    var_cov = {}\n",
    "    for number in (np.arange(len(vol))):\n",
    "        var_cov[number] = pd.DataFrame(data=None, index=betas.columns, columns=betas.columns)\n",
    "\n",
    "    for i in range (len(vol)):\n",
    "        var_cov[i] = pd.DataFrame(data=np.outer(betas[i:i+1], betas[i:i+1]) * vol[i]**2, index=betas.columns, columns=betas.columns)\n",
    "\n",
    "    for i in range (len(vol)):\n",
    "        np.fill_diagonal(var_cov[i].values, variance.iloc[i:i+1])\n",
    "        \n",
    "    return var_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical Variance Covariance Matrices; Ledoit & Wolf Shrinkage; Zero Correlation Matrix; Portfolio of Covariance Matricies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_cov (T, df_CCR, starting_dates):\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    T - the number of trading days used to calculate the estimate\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    var_cov_hist = {}\n",
    "    for number in (np.arange(len(starting_dates))):\n",
    "        var_cov_hist[number] = pd.DataFrame(data=None, index=df_CCR.columns[:-1], columns=df_CCR.columns[:-1])\n",
    "\n",
    "    for i in range (len(starting_dates)):\n",
    "        var_cov_hist[i] = ((df_CCR[(df_CCR.index<starting_dates[i])*(df_CCR.index>df_CCR.index[int(np.argwhere(df_CCR.index==starting_dates[i])-T-1)])].iloc[:, :-1]).cov())*252    \n",
    "        \n",
    "    return var_cov_hist\n",
    "\n",
    "\n",
    "def Ledoit_Wolf (T, df_CCR, starting_dates):\n",
    "    \n",
    "    var_cov_lw = {}\n",
    "    for i in range (len(starting_dates)):\n",
    "        var_cov_lw[i] = covariance.ledoit_wolf(df_CCR[(df_CCR.index<starting_dates[i])*(df_CCR.index>df_CCR.index[int(np.argwhere(df_CCR.index==starting_dates[i])-T-1)])].iloc[:, :-1])[0]  \n",
    "        var_cov_lw[i] = pd.DataFrame (data = var_cov_lw[i] * 252, index = df_CCR.columns[:-1], columns = df_CCR.columns[:-1])\n",
    "    \n",
    "    return var_cov_lw\n",
    "\n",
    "\n",
    "def zero_corr (var_cov): \n",
    "    \"keep original diagonal, set all other elements = 0\"\n",
    "\n",
    "    zero_correlation = copy.deepcopy(var_cov)\n",
    "        \n",
    "    for i in range(len(zero_correlation)):\n",
    "        diagonal = np.diagonal (zero_correlation[i])\n",
    "        zero_correlation[i] *= 0\n",
    "        np.fill_diagonal(np.asarray(zero_correlation[i]), diagonal)\n",
    "\n",
    "        \n",
    "    return zero_correlation\n",
    "\n",
    "def cov_portfolio (var_cov1, var_cov2):\n",
    "    cov_port = {}\n",
    "    for i in range(len(var_cov1)):\n",
    "        cov_port[i] = var_cov1[i] * 0.5 + var_cov2[i] * 0.5\n",
    "        \n",
    "    return cov_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_cov_ewma_implied = {}\n",
    "var_cov_hist = {}\n",
    "var_cov_lw = {}\n",
    "zero_corr_ewma_implied = {}\n",
    "cov_port = {}\n",
    "cov_port_diagvix = {}\n",
    "\n",
    "for i in range(len(data)):\n",
    "    var_cov_hist[i] = hist_cov(252, data[i], starting_dates[i])\n",
    "    var_cov_ewma_implied[i] = var_cov(model_ewma_5years, betas[i], MSE[i], starts[i])\n",
    "    var_cov_lw[i] = Ledoit_Wolf(252, data[i], starting_dates[i])\n",
    "    zero_corr_ewma_implied[i] = zero_corr (var_cov_ewma_implied[i])        \n",
    "    cov_port[i] = cov_portfolio (var_cov_lw[i], var_cov_ewma_implied[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Weights\n",
    "## Global Minimum Variance Portfolio\n",
    "weights = [inverse_var_cov @ Ix1 (column vector of ones)] / [Ix1.T @ inverse_var_cov @ Ix1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMVP (var_cov):    \n",
    "\n",
    "    inverse = {}\n",
    "    for number in (np.arange(len(var_cov))):\n",
    "        inverse[number] = np.linalg.inv(np.matrix(var_cov[number].to_numpy(), dtype=float))\n",
    "\n",
    "    ones = np.ones((len(var_cov[0].columns), 1))\n",
    "\n",
    "    weights = {}\n",
    "    for number in (np.arange(len(var_cov))):\n",
    "        weights[number] = pd.DataFrame(data=(inverse[number] @ ones)/(ones.T@inverse[number]@ones), index=var_cov[0].columns)\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_hist = {}\n",
    "weights_ewma_implied = {}\n",
    "weights_lw = {}\n",
    "weights_J_Ma = {}\n",
    "\n",
    "for i in range (len(data)):\n",
    "    weights_hist[i] = GMVP (var_cov_hist[i])\n",
    "    weights_ewma_implied[i] = GMVP (var_cov_ewma_implied[i])\n",
    "    weights_lw[i] = GMVP (var_cov_lw[i])\n",
    "    weights_J_Ma[i] = GMVP (cov_port[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to calculate inverse covariance matrix, in case of zero correlation matrix we have to incorporate some noise.\n",
    "\n",
    "def GMVP_zero_corr (zero_corr):    \n",
    "    \n",
    "    inverse = {}\n",
    "    for i in range (len(zero_corr_hist)):\n",
    "\n",
    "        for z in range (len(zero_corr_hist[i])):\n",
    "            zero_corr_hist[i][z:z+1] += 0.000001 * np.random.rand()\n",
    "\n",
    "        inverse[i] = np.linalg.inv(np.matrix(zero_corr_hist[i].to_numpy(), dtype=float))\n",
    "        \n",
    "    ones = np.ones((len(zero_corr[0].columns), 1))\n",
    "\n",
    "    weights = {}\n",
    "    for number in (np.arange(len(zero_corr))):\n",
    "        weights[number] = pd.DataFrame(data=(inverse[number] @ ones)/(ones.T@inverse[number]@ones), index=zero_corr[0].columns)\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_zero_ewma_implied = {}\n",
    "for i in range(len(data)):\n",
    "    weights_zero_ewma_implied[i] = GMVP (zero_corr_ewma_implied[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_equal = {}\n",
    "for i in range(len(data)):\n",
    "    number_of_stocks = len(data[i].columns[:-1])\n",
    "    equal = {}\n",
    "    for number in (np.arange(len(starting_dates[i]))):\n",
    "        equal[number] = pd.DataFrame(data=np.reshape((np.ones(number_of_stocks))/(number_of_stocks), (number_of_stocks,1)), index=data[i].columns[:-1])\n",
    "    weights_equal[i] = equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short Sale Constrained Minimum Variance Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_short(var_cov, df_CCR):\n",
    "    \n",
    "    x0 = (np.ones(len(df_CCR.columns)-1)/(len(df_CCR.columns)-1))\n",
    "    x0 = x0.reshape (len(df_CCR.columns)-1, 1) #initial guess 1/N\n",
    "\n",
    "    weights = {}\n",
    "    for number in (np.arange(len(var_cov))):\n",
    "        f = lambda x: np.dot(x.T, np.dot(var_cov[number], x)) #function to be minimized\n",
    "        cons = [{\"type\": \"eq\", \"fun\": lambda x: sum(x[i] for i in range (len(x)))-1}] #equallity constraint, weights need to sum up to 1\n",
    "        bound = Bounds(0, 1) #allow only positive weights\n",
    "        res = minimize(f, x0, method='SLSQP', constraints=cons, bounds=bound)      \n",
    "        weights[number] = pd.DataFrame(data=res.x, index=var_cov[number].columns)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_hist_NS = {}\n",
    "for i in range(len(data)):\n",
    "    weights_hist_NS[i] = no_short(var_cov_hist[i], data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data (name, data):\n",
    "    \"\"\"\"\"\n",
    "    name e.g. df1.pickle\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    pickle_out = open(name, 'wb')\n",
    "    pickle.dump (data, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "weights = [weights_equal, weights_hist, weights_hist_NS, weights_ewma_implied, weights_zero_ewma_implied, weights_lw, weights_J_Ma]\n",
    "portfolios = ['1/N', 'hist', 'hist_NS', 'ewma_implied', 'zero_ewma_implied', \"LW\", \"JM\", \"JM_vix\"]    \n",
    "    \n",
    "store_data ('regular.pickle', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Momentum Into Minimum Variance Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_momentum (starting_dates, Y, var_cov, df_CCR):\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    Y should be between 0.1 and 0.9, the larger the Y the more effect momentum has on the choice of optimal weights\n",
    "    \"\"\"\"\"\n",
    "\n",
    "    var_cov_momentum = copy.deepcopy(var_cov)\n",
    "    \n",
    "    for i in range (len(var_cov)):\n",
    "        lower_int = int (np.argwhere(df_CCR.index == starting_dates[i])) - 252\n",
    "        upper_int = int (np.argwhere(df_CCR.index == starting_dates[i])) - 21\n",
    "        bins = ['10', '20', '30', '40', '50', '60', '70', '80', '90', '100']\n",
    "        data = df_CCR.iloc[:, :-1]\n",
    "        deciles = pd.qcut (np.mean (data[lower_int:upper_int]) * 252, len(bins), labels=bins, retbins=True)[0]\n",
    "        \n",
    "        winners = []\n",
    "        losers = []\n",
    "        for z in range (len(deciles)):\n",
    "            if deciles[z] == '100':\n",
    "                winners.append(z)\n",
    "            elif deciles[z] == '10':\n",
    "                losers.append(z)\n",
    "                \n",
    "        winners_name = []\n",
    "        for j in range (len(winners)):\n",
    "            winners_name.append (df_CCR.columns[winners[j]])\n",
    "\n",
    "        losers_name = []\n",
    "        for j in range (len(losers)):\n",
    "            losers_name.append (df_CCR.columns[losers[j]])\n",
    "            \n",
    "        for q in range (len (winners_name)):\n",
    "            var_cov_momentum[i].iloc[[np.argwhere(var_cov_momentum[i].columns==winners_name[q])[0][0]], \n",
    "                                     [np.argwhere(var_cov_momentum[i].columns==winners_name[q])[0][0]]] *= (1-Y)\n",
    "\n",
    "        for w in range (len (losers_name)):\n",
    "            var_cov_momentum[i].iloc[[np.argwhere(var_cov_momentum[i].columns==losers_name[w])[0][0]], \n",
    "                                     [np.argwhere(var_cov_momentum[i].columns==losers_name[w])[0][0]]] *= (1+Y)\n",
    "    \n",
    "    return var_cov_momentum\n",
    "\n",
    "def adjust_bounds(var_cov, lb, ub, df_CCR):\n",
    "    \n",
    "\n",
    "    x0 = (np.ones(len(df_CCR.columns)-1)/(len(df_CCR.columns)-1))\n",
    "    x0 = x0.reshape (len(df_CCR.columns)-1, 1) #initial guess 1/N\n",
    "\n",
    "    weights = {}\n",
    "    for number in (np.arange(len(var_cov))):\n",
    "        f = lambda x: np.dot(x.T, np.dot(var_cov[number], x)) #function to be minimized\n",
    "        cons = [{\"type\": \"eq\", \"fun\": lambda x: sum(x[i] for i in range (len(x)))-1}] #equallity constraint, weights need to sum up to 1\n",
    "        bound = Bounds(lb, ub) #allow only positive weights\n",
    "        res = minimize(f, x0, method='SLSQP', constraints=cons, bounds=bound)      \n",
    "        weights[number] = pd.DataFrame(data=res.x, index=var_cov[number].columns)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cov_hist_M = {}\n",
    "cov_ewma_implied_M = {}\n",
    "cov_ewma_implied_M1 = {}\n",
    "cov_ewma_implied_M2 = {}\n",
    "for i in range(len(data)):\n",
    "    #cov_hist_M[i] = cov_momentum (starting_dates[i], 0.5, var_cov_hist[i], data[i])\n",
    "    cov_ewma_implied_M[i] = cov_momentum (starting_dates[i], 0.5, var_cov_ewma_implied[i], data[i])\n",
    "    cov_ewma_implied_M1[i] = cov_momentum (starting_dates[i], 0.7, var_cov_ewma_implied[i], data[i])\n",
    "    cov_ewma_implied_M2[i] = cov_momentum (starting_dates[i], 0.9, var_cov_ewma_implied[i], data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w_hist_M = {}\n",
    "w_ewma_implied_M = {}\n",
    "w_ewma_implied_M1 = {}\n",
    "w_ewma_implied_M2 = {}\n",
    "\n",
    "for i in range (len(data)):\n",
    "    #w_hist_M[i] = adjust_bounds (cov_hist_M[i], -1, 1, data[i])\n",
    "    w_ewma_implied_M[i] = adjust_bounds (cov_ewma_implied_M[i], -1, 1, data[i])\n",
    "    w_ewma_implied_M1[i] = adjust_bounds (cov_ewma_implied_M1[i], -1, 1, data[i])\n",
    "    w_ewma_implied_M2[i] = adjust_bounds (cov_ewma_implied_M2[i], -1, 1, data[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def momentum_only (weights_copy, starting_dates, df_CCR):\n",
    "    \n",
    "    \"\"\"\"\"\n",
    "    weights_copy - insert any of the weights already calculated, to make backtesting of momentum strategy run with the same code\n",
    "    as the regular strategies, we start by coping the format of one of the regular weights.\n",
    "    \"\"\"\"\"\n",
    "\n",
    "    winners_list = {}\n",
    "    losers_list = {}\n",
    "\n",
    "    for i in range (len(weights_copy)):\n",
    "        lower_int = int (np.argwhere(df_CCR.index == starting_dates[i])) - 252\n",
    "        upper_int = int (np.argwhere(df_CCR.index == starting_dates[i])) - 21\n",
    "        bins = ['10', '20', '30', '40', '50', '60', '70', '80', '90', '100']\n",
    "        data = df_CCR.iloc[:, :-1]\n",
    "        deciles = pd.qcut (np.mean (data[lower_int:upper_int])/ np.std(data[lower_int:upper_int]), len(bins), labels=bins, retbins=True)[0]\n",
    "\n",
    "        winners = []\n",
    "        losers = []\n",
    "        for z in range (len(deciles)):\n",
    "            if deciles[z] == '100':\n",
    "                winners.append(z)\n",
    "            elif deciles[z] == '10':\n",
    "                losers.append(z)\n",
    "\n",
    "\n",
    "        winners_name = []\n",
    "        for j in range (len(winners)):\n",
    "            winners_name.append (df_CCR.columns[winners[j]])\n",
    "\n",
    "        winners_list[i] = winners_name\n",
    "\n",
    "        losers_name = []\n",
    "        for j in range (len(losers)):\n",
    "            losers_name.append (df_CCR.columns[losers[j]])\n",
    "\n",
    "        losers_list[i] = losers_name\n",
    "        \n",
    "\n",
    "    pure_momentum = copy.deepcopy(weights_copy)\n",
    "    \n",
    "    for i in range (len(pure_momentum)):\n",
    "        pure_momentum[i].columns = ['Weights']\n",
    "        pure_momentum[i]['Weights'] = 0\n",
    "        pure_momentum[i]['Weights'][winners_list[i]] = 1\n",
    "        pure_momentum[i]['Weights'][losers_list[i]] = -1    \n",
    "        \n",
    "    return pure_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_momentum = {}\n",
    "for i in range (len(data)):\n",
    "    pure_momentum[i] = momentum_only (weights_equal[i], starting_dates[i], data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'samples' (list)\n"
     ]
    }
   ],
   "source": [
    "samples = [sample0, sample1, sample2, sample3, sample4, sample5, sample6, sample7, sample8] \n",
    "%store samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_M = [w_ewma_implied_M, w_ewma_implied_M1, w_ewma_implied_M2, pure_momentum]\n",
    "Portfolios_M = ['0.5', '0.7', '0.9', 'pure_momentum']\n",
    "\n",
    "def store_data (name, data):\n",
    "    \"\"\"\"\"\n",
    "    name e.g. df1.pickle\n",
    "    \"\"\"\"\"\n",
    "    \n",
    "    pickle_out = open(name, 'wb')\n",
    "    pickle.dump (data, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "store_data ('momentum.pickle', weights_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
